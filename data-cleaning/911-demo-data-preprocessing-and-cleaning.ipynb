{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1381403,"sourceType":"datasetVersion","datasetId":100}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Notebook: 911 Demo - Data Preprocessing and Cleaning\n# Author: Thomas Purk\n# Date: 2025-03-14\n# Reference: https://www.kaggle.com/datasets/mchirico/montcoalert","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3. Data Preprocessing & Cleaning\n- Handle **missing data**, **inconsistent formats**, and **outliers**.\n- **Feature engineering**: Create new features, encode categorical variables, normalize/scale features.\n- Split data into **training, validation, and test sets**.\n\n---","metadata":{}},{"cell_type":"code","source":"# Load the data and display a list of columns\ndf_911 = pd.read_csv('/kaggle/input/montcoalert/911.csv')\ndf_911.info()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_911.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 3.1 Inspect Each Feature","metadata":{}},{"cell_type":"code","source":"# Inspect the lat feature\nprint(\"### lat ###\")\ndf_911['lat'].describe()","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Notes on lat**\n- WGS84 Lat ranges is about 39.92845686753457,  40.49685656154363, \n- Zero latitude is the equator. These are not valid values for locations in PA\n- Postive 51 degrees latitude is too far north, in Quebec Canada\n- The mean around 40 degrees is about right for Montgomery County PA\n- So there are some incorrect values in this data","metadata":{}},{"cell_type":"code","source":"# Inspect the lat feature\nprint(\"### lng ###\")\ndf_911['lng'].describe()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Notes on lng**\n- WGS84 lng ranges is about -75.72962906989237, -74.99629164995669\n- Eastern Hemisphere always has negative longitudes\n- So there are some incorrect values in this data","metadata":{}},{"cell_type":"code","source":"# Inspect the desc feature\nprint(\"### desc ###\")\ndf_911['desc'].describe()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Inspect split of desc feature\ndesc_split = df_911['desc'].str.split(';', expand=True)\ndisplay(desc_split.info())\ndisplay(desc_split.head())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Notes on desc**\n- string feature\n- appears to contain several values\n- Values may be repeated in other features\n- But Station name may be unique","metadata":{}},{"cell_type":"code","source":"# CLEAN UP \"desc\" - Remove substrings from desc that are repeated in other fields\n\n# The goal is to find valuable data in the desc field if any\n\n# Copy the data\ndf_911_v1 = df_911.copy()\n\n# Loop through rows using iterrows()\nfor index, row in df_911_v1.iterrows():\n    # Remove addr string\n    # Have some bad addr value that are just integers\n    if(not row['addr'].isdigit()):\n        new_value = row['desc'].replace(row['addr'], '')\n\n    \n    # Remove twp string\n    new_value = new_value.replace(str(row['twp']), '')\n    # The Desc feature contains timestamps Example: \"2015-12-10 @ 17:10:52\"\n    # But the timeStamp feature is formated as  \"2015-12-10 17:10:52\"\n    # So split the timeStamp by ' ' and remove each part seperately\n    ts_parts = row['timeStamp'].split(' ')\n    new_value = new_value.replace(ts_parts[0], '')\n    new_value = new_value.replace(ts_parts[1], '')\n    # clean up delimiters\n    new_value = new_value.replace(';','')\n    new_value = new_value.replace('@','')\n    new_value = new_value.replace('-','')\n    new_value = new_value.replace(':','')\n    # \"Station\" and \"STA\" are not needed\n    new_value = new_value.replace('Station','')\n    new_value = new_value.replace('STA','')\n    new_value = new_value.strip()\n    # Write the remaining string as a new feature\n    df_911_v1.at[index, 'station'] = new_value","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f'desc Null count: {df_911_v1.desc.isnull().sum()}')\nprint(f'desc \"\" count: {(df_911_v1.desc == \"\").sum()}')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"s = df_911_v1[df_911_v1['desc'] != ''][['desc','twp']].value_counts()\ns","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for value, count in df_911_v1[['station', 'addr']].value_counts().items():\n    print(f\"{value}: {count}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_911[df_911_v1['desc'] == '1:05:44']","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 4. **Exploratory Data Analysis (EDA)**\n- Analyze data distributions, correlations, and patterns.\n- Visualize data (e.g., histograms, scatter plots, heatmaps).\n- Understand relationships and potential biases in the dataset.\n\n---","metadata":{}},{"cell_type":"code","source":"df['twp'].value_counts()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"keys = df['twp'].value_counts().keys()\nkeys = keys.sort_values()\nfor k in keys:\n    print(k)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(df)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}